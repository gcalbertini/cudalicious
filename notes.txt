abstract class: a class that can only be used as a base class and uses pure virtual functions in order to do so
No objects of an abstract class can be created (except for base subobjects of a class derived from it) and no non-static 
data members whose type is an abstract class can be declared. Pointers and references to an abstract class can be declared.
=========================
virtual function: specifies that a non-static member function is virtual and supports 
dynamic dispatch, i.e., can be redefined in derived class *if needed* (override optional)
========================
pure virtual method:  virtual returnType foo() = 0; or virtual returnType g() override = 0 so
*required* to be implemented by a derived class if the derived class is not abstract.
=========================
concrete class: opposite of ABC and can create a class; can be derived from ABC
=========================
protected members: prevents us from making objects of this type directly; accessible in the class that defines them and in classes that inherit from that class
=========================
private members: only accessible within the class defining them
=========================
friends of their derived classes: both protected and private are available to these classes they are friends to (can be a global function or member function of another class)
friend return_type function_name (arguments);    // for a global function
            or
friend return_type class_name::function_name (arguments);    // for a member function of another class
========================
constructor: is a special non-static member function of a class that is used to initialize objects of its class type
========================
destructor: is a special member function that is called when the lifetime of an object ends. 
The purpose of the destructor is to free the resources that the object may have acquired during its lifetime. 
========================
delete keyword prevente copy constructor and copy assignment.
obj(const obj&) = delete // prevents copying
obj& operator=(const obj&) = delete // prevents assignment
========================
operator overload: T& operator=(const T& other) for copy assignment for example
========================
inheritance:  inheritance is the mechanism of basing an object or class upon another object 
(prototype-based inheritance) or class (class-based inheritance), 
retaining similar implementation. Also defined as deriving new classes (sub classes) 
from existing ones such as super class or base class and then forming them into a 
hierarchy of classes. In most class-based object-oriented languages like C++, an object 
created through inheritance, a "child object", acquires all the properties and behaviors
 of the "parent object", with the exception of: constructors, destructors, overloaded 
 operators and friend functions of the base class.
==========================
general polymorphism: concept that you can access objects of different types through the same interface. Each type can provide its own independent implementation of this interface.
==========================
Run-time polymorphism/dynamic dispatch/run-time dispatch/late binding: function called is determined at run-time based on object used (for virtual function methods)
==========================
Compile-time/static dispatch/polymorphism: for operator overloading and function overloading where signatures are changed but functions have the same name 
==========================
encapsulation: keeping implementation details of our classes private to protect them from direct use that could complicate maintenance
thus prevents external code from being concerned with the internal workings of an object; encourages decoupling
==========================
vtable: how most C++ implementations do polymorphism. For each concrete implementation of a class,
there is a table of function pointers ( _vptr is the hidden pointer) to all the virtual methods. A pointer to this table 
(called the virtual table) exists as a data member in all the objects. When one calls a virtual 
method, we lookup the object's v-table and call the appropriate derived class method.
==========================
template: mechanism which allows a programmer to use types as paramters for a class or a function. 
the compiler then generates a specific class or function when we provide specific types as arguments 
==========================
metaprogramming: writing code that writes more code for you at compile-time like using templates.
With these techniques, you can do things like compute factorials or construct complicated dynamic types that present high level data, 
all without any run-time overhead 
==========================
static cast: compile time; does things like implicit conversions between types 
(such as int to float, or pointer to void*), and it can also call explicit conversion functions.
even if you think you can somehow typecast a particular object pointer into another but it’s illegal, 
the static_cast will not allow you to do this; to use static_cast in case of inheritance, the base class must be accessible, 
non virtual and unambiguous. static_cast operator allows casting from any pointer type to void pointer and vice versa.
static_cast <dest_type> (source);
===========================
dynamic cast: In C++, dynamic casting is mainly used for safe 
downcasting at run time. To work on dynamic_cast there must be one virtual function in the base 
class. A dynamic_cast works only polymorphic base class because it uses this information to 
decide safe downcasting. Casting a base class pointer (or reference) to a derived class pointer 
(or reference) is known as downcasting. Casting a derived class pointer (or reference) to a base class pointer 
(or reference) is known as upcasting. Now, If the cast fails and new_type is a pointer type, it returns a null 
pointer of that type.
============================
reinterpret cast: It is used to convert a pointer of some data type into a pointer of another data type, even if the data types before and after conversion are different.
t does not check if the pointer type and data pointed by the pointer is same or not. It doesn’t have any return type. It simply converts the pointer type.
============================
============================
STL containers
===========================
vector: contiguously allocated sequence of elements; use as default container
list: doubly-linked list; used when need to insert and delete elem without moving existing ones
deque: mix of list and vector; don't use until have expert knowledge of algos and machine arch
map: a balanced ordered tree; use when needing to access elements by value
multimap: balanced ordered tree where there can be multiple copies of a key
unordered_map: a hash table; optimized version of map used for large maps when you need high perf and can create a good hash function
unordered_multimap: as above but multiple copies of a key
set: balanced ordered tree; use when need to keep track of individual values
multiset: as above but multiple keys 
unordered_set: like unordered_map but just with values not (k,v) pairs
unordered_multiset: like unordered_multimap, but just with values not (k,v) pairs
array: fixed size array that doesnt suffer most problems as built-in variants
============================
DATA STRUCTURES
============================
ARRAY 
Values contiguous in RAM. As long as we know index order of elements, can insert and remove in O(1) best case
To insert/remove into middle O(n) as need to shift elements overhead
==============================
LINKED LIST 
Values are not contiguous in RAM but connected by pointers. To insert/remove from the end O(1) as we have nullptr at ends
To insert into middle, assuming you know address, can detach and reaatach pointers for O(1), else O(n) to count from beginning to where you want to be
==============================
HASH MAP 
Instead of a key, have an index [0, n-1] not necessarily evenly (NOT ORDERED) spaced or ints (can have chars), 
then map each to some value with a hash function.
Can insert and remove in O(1) as we just find the key and drop or just add to the pile as is unordered.
Searching for key also O(1) 
==================================
QUEUE 
A doubly-linked-list. Push/pop from either front or back is O(1). Typically push elements to the back and pop from the front. 
===========================
BINARY TREE 
Typically a tree map where each node has a key value mapped to true value of node. For every child node 
spawned to the left of the parent node, value is less than that of parent's. For child nodes spawned to the right,
we get nodes with higher values. 
For a balanced tree: insert/remove/search all done in O(log n) time. What makes it balanced? 
For each level, the height of the left subtree is the same as the right one, 
so every root at that level has either two nodes or one node. 
Values are ordered so if you DFS, can get ordered list of elements. 
For unbalanced tree: built from sorted data is effectively the same as a linked list as elem increase so O(n) search worst case
=============================
Trie/Prefix tree
Each node usually represents a single character and has up to 26 children, one of each chars in alpha
To insert "ANT" having A->N connection, then just need additional T node. Insert/search then O(n)
Helpful when you know the prefixes to words
====================
HEAP
Usually min/max heap where min/max value will be the root of the tree and the children will always be greater/lesser. 
Every level of tree will be full except for maybe last level. While visualized as trees Typically
implement as arrays under the hood: [x|3|6|8|9] to get left 2*i and to get right 2*i+1 for index i
To get min/max value then O(1) as its parent node. To pop min/max value thne O(logN) also for insertion
======================
GRAPH
Nodes with edges that connect them together. Can be directed (edges have directions) or not to establish paths. Can be 
represented by adjacency list (Node-Neighbors list) when very dense 

==================
==================
A hashmap (or unordered_map in C++ terminology) is a data structure that maps keys to values. 

A hashmap typically consists of an array (or a table) of buckets, and each bucket may contain a linked list or
 another data structure to handle collisions. Here are the main components:
    1. Array of Buckets: The hashmap has an array of buckets, and the size of this array is usually
        determined based on the expected number of elements and the desired level of performance.
    2. Bucket: Each element in the array is a bucket, and it serves as a container for key-value pairs. The bucket may contain
        a linked list, a binary search tree, or another structure to handle multiple key-value pairs that hash to the same index (collisions).
    3. Key-Value Pairs: Each key-value pair represents the data stored in the hashmap. The key is used to calculate the hash code, which is then
        used to determine the index (bucket) where the key-value pair should be stored.
    4. Hash Function: A hash function is used to convert a key into an index within the array of buckets. The goal of a good 
        hash function is to distribute keys evenly across the array, reducing the likelihood of collisions.
+-------------------------------------+
| Array of Buckets                    |
+-------------------------------------+
| Bucket 0                            |
|   |-> (Key1, Value1)                |
|   |-> (Key2, Value2)                |
|   |-> ...                           |
| Bucket 1                            |
|   |-> (Key3, Value3)                |
|   |-> ...                           |
| ...                                 |
| Bucket N                            |
|   |-> ...                           |
+-------------------------------------+

When you want to insert or retrieve a key-value pair, the hashmap calculates the hash code of the key, 
maps it to an index in the array, and then searches or inserts the key-value pair in the corresponding bucket.

It's important to note that modern hashmaps often include optimizations, such as dynamically resizing the array
 to maintain a balance between load factor and performance. The load factor is the ratio of the number of stored 
 elements to the number of buckets, and keeping it within a certain range helps to prevent performance 
 degradation due to collisions.
=============================
==============================
A process and a thread are both fundamental concepts in operating systems and concurrent programming, but they serve
 different purposes and have distinct characteristics. Here are the key differences between a process and a thread:

    Definition:
        Process: A process can be viewed as an independent program in execution. It has its own memory space, resources, and state. 
        Processes are typically isolated from each other and communicate through inter-process communication mechanisms.

        Thread: A thread is the smallest unit of execution within a process. Threads share the same memory space and resources, 
        such as file handles, with other threads in the same process. Multiple threads within a process can run concurrently.

    Memory Space:
        Process: Each process has its own address space, which includes code, data, and system resources. 
        Processes are isolated from each other, and communication between processes usually involves inter-process communication mechanisms.

        Thread: Threads within the same process share the same address space. This means they can directly communicate with each other through 
        shared variables and can access the same data structures.

    Resource Overhead:
        Process: Processes have higher resource overhead compared to threads because each process has its own memory space, 
        file descriptors, and other resources.

        Thread: Threads have lower resource overhead since they share resources with other threads in the same process.

    Communication:
        Process: Inter-process communication mechanisms, such as message passing or shared files, are used for communication between processes.
        Thread: Threads within the same process can communicate more easily through shared memory since they have access to the same data structures.

    Creation and Termination:
        Process: Processes are typically heavier to create and terminate. Creating a new process involves duplicating the existing process, 
        including its memory space.

        Thread: Threads are lighter weight and quicker to create and terminate compared to processes.

    Independence:
        Process: Processes are independent of each other, and the failure of one process does not directly affect others.

        Thread: Threads within the same process are more interdependent. If one thread crashes or encounters an error, 
        it can potentially affect the entire process.

In summary, a process is an independent program with its own memory space, while a thread is a 
unit of execution within a process that shares the same memory space with other threads in that process. 
Processes are more isolated, have higher resource overhead, and communicate through inter-process communication mechanisms, 
while threads share resources and communicate more easily through shared memory.
=======================
======================
Purpose:

    HashMap: A hashmap (or unordered_map in C++, for example) is designed to store key-value pairs, where each key is unique. 
    It uses a hash function to map keys to indices in an array, allowing for efficient insertion, retrieval, and deletion of key-value pairs.

    Set: A set is a collection of unique elements, and it only stores values without associated keys. The primary goal of a set is to provide a
     way to check for the presence or absence of an element in the collection.

Structure:

    HashMap: A hashmap is typically implemented as an array of buckets, and each bucket can contain a linked list, a binary search tree, 
    or another structure to handle collisions (multiple keys hashing to the same index).

    Set: A set is commonly implemented as a hashmap where only keys are used (with a dummy value, for example) or as a specialized data 
    structure optimized for efficient membership testing, such as a balanced tree.

Key Characteristics:

    HashMap: Requires both a key and a value. The key is used for indexing and the value represents the associated data.

    Set: Only contains the elements (values), and it is concerned with checking for the presence or absence of these elements.

Usage:

    HashMap: Used when you need to associate a value with each key, and you want efficient key-based lookups. 
    Useful for scenarios where you need to store and retrieve data based on some identifier.

    Set: Used when you only need to maintain a unique collection of elements and do not need to associate any additional 
    information (values) with those elements.

std::unordered_map<int, std::string> myHashMap;
myHashMap[1] = "One";
myHashMap[2] = "Two";
myHashMap[3] = "Three";

std::unordered_set<int> mySet;
mySet.insert(1);
mySet.insert(2);
mySet.insert(3);

In summary, a hashmap is a key-value store, while a set is focused on maintaining a unique collection 
of elements without associated values. Hashmaps can be used to implement sets,
 but the choice between them depends on whether you need to associate additional data (values) with your keys 
 or if you simply want to manage a collection of unique elements.
===============
===============
References and pointers are both used in C++ to indirectly access variables, but they have some key differences:

Reference: Once a reference is declared, it must always refer to the same object. 
The syntax for declaring a reference is simpler than that of a pointer.

int x = 10;
int& ref = x;  // Declaration and initialization of a reference

Pointer: A pointer can be re-assigned to point to different objects during its lifetime.
 The syntax for declaring a pointer involves the use of an asterisk.

    int x = 10;
    int* ptr = &x; // Declaration and initialization of a pointer

Nullability:

    Reference: Cannot be null. It must always refer to a valid object.

    Pointer: Can be assigned a null value (nullptr in modern C++ or NULL in older versions). This allows pointers to represent the absence of an object.

Memory Management:

    Reference: Once a reference is bound to an object, it cannot be rebound to refer to a different object. 
    References don't need to be explicitly deallocated, and there is no "null reference."

    Pointer: Pointers can be reassigned to point to different objects, and memory allocated using pointers needs to be explicitly 
    deallocated to avoid memory leaks.

Indirection (Dereferencing):

    Reference: No explicit dereferencing is needed. Accessing a reference is similar to accessing the actual object.

int x = 10;
int& ref = x;
int value = ref; // No explicit dereferencing

Pointer: Requires explicit dereferencing using the asterisk (*) operator to access the value pointed to by the pointer.

        int x = 10;
        int* ptr = &x;
        int value = *ptr; // Explicit dereferencing

    Reassignment:
        Reference: Cannot be reassigned to refer to a different object once initialized.

        Pointer: Can be reassigned to point to different objects during its lifetime.

    Size:
        Reference: Typically takes up the same amount of memory as the type it refers to. No separate memory is allocated for a reference.

        Pointer: Requires additional memory to store the memory address it points to.

In summary, references provide a simpler syntax, are non-nullable, and have automatic dereferencing, 
while pointers offer greater flexibility with reassignment, nullability, and explicit dereferencing. 
The choice between them depends on the specific requirements of your program.
=================
std::unique_ptr and std::shared_ptr are smart pointers in C++ that provide automatic memory management and ownership semantics. Here are the key differences between them:

    Ownership:
        std::unique_ptr: It represents sole ownership of a dynamically allocated object. 

        There can be only one std::unique_ptr that owns a particular object. When a std::unique_ptr is moved or reset,
         it transfers ownership to the new std::unique_ptr, and the original pointer becomes null.

        std::shared_ptr: It represents shared ownership of a dynamically allocated object. 
        Multiple std::shared_ptr instances can share ownership of the same object. The object is only deleted when the last std::shared_ptr pointing to it is destroyed or reset.

    Performance Overhead:
        std::unique_ptr: Generally has lower performance overhead than std::shared_ptr.
         It is a lightweight smart pointer designed for scenarios where exclusive ownership is sufficient.

        std::shared_ptr: Has additional overhead to manage the reference count, 
        leading to slightly higher performance costs compared to std::unique_ptr.

    Memory Overhead:
        std::unique_ptr: Typically has lower memory overhead than std::shared_ptr. 
        It only needs to store a pointer and potentially a deleter function.

        std::shared_ptr: Requires additional memory to store the reference count,
         leading to higher memory overhead compared to std::unique_ptr.

    Use Cases:
        std::unique_ptr: Preferred when exclusive ownership is required, and there is no need for shared ownership. 
        It is more suitable for scenarios where ownership is transferred, such as returning a pointer from a function or using it in a container.

        std::shared_ptr: Suitable when multiple entities need to share ownership of an object. It is commonly used in scenarios like circular 
        dependencies or scenarios where it's challenging to determine the exact lifetime of objects.

    Move Semantics:
        std::unique_ptr: Supports move semantics. When a std::unique_ptr is moved, ownership is transferred, and the source pointer becomes null.

        std::shared_ptr: Also supports move semantics. When a std::shared_ptr is moved, the ownership 
        is shared between the source and the destination, and the source pointer remains valid.

    Thread Safety:
        std::unique_ptr: Not inherently thread-safe. Designed for exclusive ownership in a single-threaded or 
        carefully synchronized environment.

        std::shared_ptr: Provides built-in reference counting, making it thread-safe. 
        Multiple std::shared_ptr instances can safely share ownership across multiple threads.

In summary, choose between std::unique_ptr and std::shared_ptr based on your specific ownership requirements. 
If exclusive ownership is sufficient, and you want to minimize performance and memory overhead, use std::unique_ptr. 
If shared ownership is necessary and you need the automatic memory management features of reference counting, use std::shared_ptr.

Sorting algos and time complexity: https://www.geeksforgeeks.org/time-complexities-of-all-sorting-algorithms/


MULTITHREEADING AND CUDA; Include AI algo Qs!


What is reume project?

PLEASE SEE SLIDES SUBMIT FOR HPS AS HE WILL ASK (HOGWILD)
PLEASE SEE PRM MODEL AND LAMBDAMART AS HE WILL ASK
COCO DATASET AND IMAGE SEGMENTATION AS HE WILL ASK 

LATEST NEWS ON NVIDIA WAS LINKED. PLEASE ASK RELEVANT RECENT
QS ALONGSIDE ONES NOTED:

How are GPUs launching kernels on their own now? (huh)
hUMA and cahce coherence between CPU + GPU (what rivals AMDs APU)
Nvidia RAPIDS cuDNN libraries 
WHy not move to Rust or other langs yet?
Others on his background
Why this team; what the goal will ultimately be
What this role specifically does 

Expanding team and work loads they deal with. Maximizing performance 
out of applications. Traditionally GPU just used in R&D/HPC settings with super computers
Over the last 5 years going into AI --> optimize deep learning algos for inference/training 
or implementing traditional ML algo in parallel

How do you choose what to work on? External partners buy GPUs, come to you and want to run X model
on GPU but not within latency budgets within application --> work with them to figure out
SW solution or how to efficiently parallelize the process they are looking at or how to distribute
it across multiple nodes 

Sometimes internal projects to make efficient CUDA code or general || code 

Expanding team --> 5-10 yrs of experience on the team

Learn the workloads, see what users are using it and what they require --> internalize some learnings from them
Team collabs with hardware teams thinking of new features (2-5 yrs down the line) to future-proof algos

Third aspect, evangelizing CUDA and NVIDIA ecosystem, writing up the technical blogs and speaker sessions
at conferences that make it through conf submissions --> help further || computing with the green team

Because we are with devs and CUDA a lot, work with SW/HW new programming models to expose new features 
One stop shop for parallel computing team (touch eveything based on experience and interests)

NVIDIA research occassional collabs but normally submit a paper to conference based on the output of work
But more focused on HW/SW interactions 

Expanding due to work for LLM demand; GPU space for DL --> new compilers demand new kernels 
and techniques; new precision and new techniques 


How will the team collaborate with other existing teams within the organization?
What skill sets are you looking for when building the team, both in terms of CUDA expertise and broader software and hardware skills?
Are there specific roles or positions that need to be filled within the team? Size? 
Will there be opportunities for team members to receive training or professional development in CUDA programming and optimization techniques?
Will the team collaborate with external partners, vendors, or research institutions for certain aspects of the project?


PRM Model -- Transformer self attention to home search before GPT hype on the PRM model:
Self-attention, on the other hand, allows a model to weigh the importance of different elements within the same input sequence. 
This is particularly useful for capturing relationships and dependencies between different elements in a sequence, regardless of their positions.
===========================================================================
What is atomic op:
    Indivisibility: An atomic operation is performed as a single, uninterruptible unit. It is not 
    possible for other threads or processes to observe an intermediate state of the operation.

    No Interruption: Once initiated, an atomic operation completes without being interrupted by other 
    operations. This is important for maintaining consistency and avoiding conflicts in a concurrent environment.

    Consistency: Atomic operations ensure that shared data remains in a consistent state,
     even when multiple threads are accessing or modifying it simultaneously.

    No Race Conditions: Atomic operations help prevent race conditions, where the outcome of operations
     depends on the timing or interleaving of multiple threads. By providing atomicity, these operations 
     eliminate the need for explicit locks in certain scenarios.
===========================================================================
Here's a simplified explanation of how self-attention works:

    Key, Query, and Value: In self-attention, for each element in the input sequence,
     there are three vectors associated with it: Key (K), Query (Q), and Value (V).
      These vectors are learned during the training process.

    Score Calculation: The model calculates a score for each element in
     the sequence based on the similarity between its Query vector and the Key
      vectors of all other elements. The scores determine how much attention each 
      element should pay to others.

    Weighted Sum: The scores are used to calculate weights, which are 
    applied to the corresponding Value vectors. These weighted values are
     then summed to obtain the output representation for each element.

    Multi-Head Attention: To capture different aspects or types of relationships, 
    the self-attention mechanism is often used with multiple sets of Key, Query, and Value vectors, 
    called "heads." The results from these multiple heads are concatenated and linearly transformed 
    to produce the final output.

The self-attention mechanism allows the model to consider the entire context of the input 
sequence when making predictions for each element. This capability has proven beneficial for 
tasks where understanding the relationships between different parts of the input is crucial, 
such as in language understanding and generation.
===========================================================================

What is bandwidth? Formula?

Bandwidth - the rate at which data can be transferred. 
Clock Rate x vector/8 x N <<<N = 2 for double datta rate>>> / 10e9 = GB/s

Effective bandwidth is ((Br (number bytes read/kernel) + 
Bw (number of bytes written per kernel)) /10e9 ) / time

So for MxM matrix of floats (4 bytes) and needing to read/write (2)
we have (M**2x4x2)/10e9/time
===========================================================================

What is a GAN vs CNN vs RNN vs LSTM?

    GAN (Generative Adversarial Network):
        Purpose: GANs are generative models designed to generate new data samples that resemble a given dataset.
        Architecture: GANs consist of two neural networks, a generator, and a discriminator, which are trained 
        simultaneously through adversarial training. The generator generates synthetic data, and the 
        discriminator evaluates whether the generated data is real or fake.
        Use Cases: Image generation, style transfer, data augmentation, and more.

    CNN (Convolutional Neural Network):
        Purpose: CNNs are primarily used for image-related tasks and spatial data.
        Architecture: CNNs use convolutional layers that automatically learn spatial hierarchies of features from input data. 
        They are well-suited for tasks involving images and patterns in grid-structured data.
        Use Cases: Image classification, object detection, facial recognition, and image segmentation.

    RNN (Recurrent Neural Network):
        Purpose: RNNs are designed for sequence data, where the order of data points matters.
        Architecture: RNNs have recurrent connections that allow them to maintain a memory of previous inputs. 
        This makes them suitable for tasks where context or temporal dependencies are crucial.
        Use Cases: Natural language processing, time series analysis, speech recognition.

    LSTM (Long Short-Term Memory):
        Purpose: LSTMs are a type of RNN designed to address the vanishing gradient problem and capture long-term dependencies in sequence data.
        Architecture: LSTMs have specialized memory cells that can store and retrieve information for extended periods, allowing them to handle 
        long-term dependencies in data.
        Use Cases: Similar to RNNs, LSTMs are used in natural language processing, time series analysis, and any task where long-range dependencies are critical.

In summary:

    GANs are for generating new data samples.
    CNNs are for tasks involving spatial data, especially images.
    RNNs are for sequence data where order matters.
    LSTMs are a type of RNN designed to address long-term dependencies.

===========================================================================
Difference between thread and a process?

    Definition:
        Process: A process is an independent program in execution. It has its memory space, 
        resources, and a dedicated instance of the program counter.
        Thread: A thread is the smallest unit of execution within a process. 
        Threads share the same memory space and resources of a process, but each 
        thread has its own execution context, including a separate program counter and register set.

    Independence:
        Process: Processes are independent of each other. They run in isolation and have their own memory space.
        Thread: Threads within the same process share the same memory space and resources. 
        They are more lightweight than processes and can communicate with each other more easily.

    Communication:
        Process: Inter-process communication (IPC) is required for communication between different processes. 
        This communication may involve more overhead.
        Thread: Threads within the same process can communicate directly through shared memory, making 
        communication more efficient. However, proper synchronization mechanisms are needed to avoid conflicts.

    Resource Overhead:
        Process: Processes have higher resource overhead as they are independent entities 
        with their own memory space, file handles, and other resources.
        Thread: Threads have lower resource overhead since they share resources with other 
        threads in the same process. However, they still have their own execution context.

    Creation and Termination:
        Process: Creating and terminating processes is generally slower and more resource-intensive.
        Thread: Creating and terminating threads is faster and requires less overhead.

    Fault Tolerance:
        Process: Processes are more fault-tolerant. If one process fails, it does not directly affect other processes.
        Thread: Threads within the same process are less fault-tolerant. If one thread crashes, it may affect the entire process.

    Scalability:
        Process: Processes are considered heavyweight, and creating many processes may not be as scalable.
        Thread: Threads are considered lightweight, and creating many threads within a process is generally more scalable.

In summary, a process is an independent program in execution with its own memory space, 
while a thread is a smaller unit of execution within a process that shares the same resources
 with other threads in the same process. Threads are lighter-weight than processes and can be more 
 efficient for concurrent programming but require careful synchronization to avoid conflicts.
===========================================================================
Difference between parallelism and concurrency?
Concurrency refers to the ability of a system to execute multiple independent
 tasks or processes at the same time. It doesn't necessarily mean that tasks are executing 
 simultaneously (parallel), but rather that progress is being made on multiple tasks over a period of time. 
 In concurrent systems, tasks may be interleaved, running concurrently but not necessarily in parallel.

 In summary, concurrency is about managing multiple tasks and making progress on them, potentially 
 interleaving their execution. Parallelism is about executing multiple tasks simultaneously, often to 
 improve performance. Concurrency is a broader concept that encompasses parallelism, but not all concurrent
  systems are necessarily parallel.
===========================================================================
What is a mutex?

A mutex, short for mutual exclusion, is a synchronization mechanism used in concurrent programming to ensure
 that only one thread can access a shared resource or critical section at any given time. The purpose of a 
 mutex is to prevent data corruption or unexpected behavior caused by multiple threads accessing shared resources concurrently.

Mutexes provide a way for threads to coordinate and avoid conflicts by allowing only one thread at a time to 
enter a critical section of code. When a thread wants to access the shared resource, it must first acquire the mutex.
 If the mutex is already held by another thread, the requesting thread may be blocked or made to wait until the mutex is released by the owning thread.

The basic operations on a mutex are:

    Lock (Acquire): A thread tries to acquire the mutex. If the mutex is not already held by another thread, 
    it is locked, and the thread can proceed. If the mutex is already locked, the thread may be blocked until
    it can acquire the lock.

    Unlock (Release): The owning thread releases the mutex, allowing other threads to acquire it.
===========================================================================
What is Amdahl's law? What law rose to challenge it? What is strong scaling? 

Strong scaling is a measure of how, for a fixed overall 
problem size, the time to solution decreases as 
more processors are added to a system. An application
that exhibits linear strong scaling has a speedup 
equal to the number of processors used.

Strong scaling is usually equated with Amdahl’s Law, 
which specifies the maximum speedup that can be 
expected by parallelizing portions of a serial program. 
Essentially, it states that the maximum speedup S of 
a program is:

S = 1/((1-P) + P/N)

for P fraction of total serial execution taken by portion 
of code that can be || and N is number of processors for
which || returns

Larger the N smaller the P/N fraction --> S = 1/(1-P)
===========================================================================

Gustafson Law? Weak scaling?

Weak scaling is a measure of how the time to solution 
changes as more processors are added to a system with
 a fixed problem size per processor; i.e., where the 
 overall problem size increases as the number of 
 processors is increased.

Weak scaling is often equated with Gustafson’s Law,
 which states that in practice, the problem size 
 scales with the number of processors. Because of this, 
 the maximum speedup S of a program is:

 S = N + (1-P)(1-N)

 Here P is the fraction of the total serial execution 
 time taken by the portion of code that can be parallelized
  and N is the number of processors over which 
  the parallel portion of the code runs.

Another way of looking at Gustafson’s Law is that it 
is not the problem size that remains constant as we
 scale up the system but rather the execution time. 
 Note that Gustafson’s Law assumes that the ratio of 
 serial to parallel execution remains constant, reflecting additional cost in setting up and handling the larger problem.


When would it make sense to use a detached (daemon) thread? 
When theres another mechanism for ID when thread has completed or where the thread
is used in a fire-and-forget task
===========================================================================
What is an SM?
parallel processing unit that consists of multiple CUDA cores. These cores are responsible for executing parallel threads.
The GPU allocates blocks to the SMs. The SMs run in || and independently and control simple processors. A thread block 
may not run on more than one SM 
===========================================================================
How does a GAN work? What about a CNN? Example?
===========================================================================
What is Bayesian inference?
Bayesian inference is a statistical method that provides a 
framework for updating beliefs or making predictions about uncertain events 
based on prior knowledge and new evidence.

Prior Probability (Prior):

    The initial belief or probability distribution representing what is 
    known before new data is observed. It encapsulates existing knowledge or 
    assumptions about the parameters or events under consideration.

Likelihood:

    The probability of observing the data given a specific set of parameters. 
    It describes the likelihood of the observed data under different hypotheses.

Posterior Probability (Posterior):

    The updated probability distribution representing the beliefs about the 
    parameters or events after considering the observed data. It is obtained by combining the 
    prior knowledge and the likelihood of the data.

Bayes' Theorem:

    The mathematical formula that governs Bayesian inference is Bayes' theorem:
    P(parameters∣data)=P(data∣parameters)×P(parameters)/P(data)
        P(parameters∣data) is the posterior probability.
        P(data∣parameters) is the likelihood.
        P(parameters) is the prior probability.
        P(data) is the marginal likelihood (normalizing constant).

Posterior Predictive Distribution:

    After obtaining the posterior distribution, it can be used to make predictions
     about future observations or unobserved quantities.
===========================================================================
What is MLE?

The basic idea behind MLE is to find the values of the model parameters that maximize the likelihood function, 
which measures how well the model explains the observed data.

Here are the key concepts related to Maximum Likelihood Estimation:

    Likelihood Function:
        The likelihood function, denoted as L(θ∣data), 
        represents the probability of observing the given data under a particular set 
        of parameters (θ) in the statistical model. It is essentially the probability distribution 
        of the observed data, treated as a function of the parameters.

    Log-Likelihood Function:
        The log-likelihood function, denoted as ℓ(θ∣data)ℓ(θ∣data), is often used instead of the likelihood function. 
        Taking the logarithm can simplify computations, and the logarithm does not change the location of the maximum value.

    Maximum Likelihood Estimation:
        MLE aims to find the values of the model parameters (θ^) that maximize the likelihood function (or log-likelihood function) given the observed data:
        θ^=arg⁡max_⁡ θ L(θ∣data) OR θ^=argmax_θ l(θ∣data)

        In practice, this often involves finding the values of the parameters for which the derivative (gradient) of the log-likelihood function is equal to zero.

    Interpretation:
        The MLE provides estimates of the model parameters that make the observed data most probable under the assumed statistical model.

    Consistency and Efficiency:
        Under certain conditions, MLE estimates are consistent, meaning that as the sample size increases, the estimates converge to the true parameter values.
        MLE estimates are also asymptotically efficient, meaning that they achieve the best possible estimation precision as the sample size becomes large.

    Invariance Property:
        MLE has the invariance property, which means that if θ^ is the MLE of θ, then g(θ^) is the MLE of g(θ) for any function g(⋅).

        Good for param fitting
===========================================================================
What is L2/L1 regularization? A kernel for this?

L2 (ridge) and L1 (lasso) regularization are techniques used in machine learning to prevent 
overfitting by adding a penalty term to the cost function based on the magnitude of the model parameters.
 These regularization techniques are commonly applied in linear regression and logistic regression models.

L2 Regularization (Ridge):

    Purpose: Introduces a penalty term proportional to the squared magnitude of the model parameters.
    Regularization Term: λ ∑θ_i^2​, where λ is the regularization strength and θ_i​ are the model parameters.
    Effect: Encourages small but non-zero values for all parameters, preventing any single parameter from dominating the model.

L1 Regularization (Lasso):

    Purpose: Introduces a penalty term proportional to the absolute value of the model parameters.
    Regularization Term: λ∑∣θ_i∣, where λ is the regularization strength and θ_i​ are the model parameters.
    Effect: Encourages sparsity by driving some parameter values to exactly zero, effectively performing feature selection.

===========================================================================
What is a shared and unique pointer in C++?
===========================================================================
Name commons C++ DS? Implement one or two for me. 

std::vector is a dynamic array that can grow or shrink in size. It provides constant time access to elements and efficient insertion/deletion at the end.
std::list is a doubly-linked list that allows efficient insertion and deletion of elements at any position, but with slower random access compared to vectors.
std::deque is a double-ended queue that supports fast insertion and deletion at both ends. It is similar to a vector but allows efficient insertion/deletion at the front.
std::queue is a queue data structure that follows the First In, First Out (FIFO) principle.
std::stack is a stack data structure that follows the Last In, First Out (LIFO) principle
std::set is an ordered set that stores unique elements in a sorted order
std::map is an associative container that stores key-value pairs in a sorted order based on the keys.
std::unordered_set and std::unordered_map are unordered versions of set and map, providing faster average lookup times.
===========================================================================
Describe the mem hierarchy.

All threads of a block can access the same variable in that blocks shared mem.
Threads from two different blocks can access the same variable in global mem.
Threads from different blocks have their own copy of local vars in local mem.
Threads from the same block have their own copy of local vars in local mem. 
Thread local, block shared, kernel global
==========================
Precision:

    Definition: Precision is the ratio of correctly predicted positive observations 
    (True Positives) to the total predicted positives (sum of True Positives and False Positives). 
    It measures the accuracy of the positive predictions.
    Formula: Precision= TP / TP + FP
    Objective: A high precision indicates that a positive prediction from the model is likely to be correct.

Recall (Sensitivity or True Positive Rate):

    Definition: Recall is the ratio of correctly predicted positive observations (True Positives) 
    to the total actual positives (sum of True Positives and False Negatives). It measures the ability of the model to capture all the positive instances.
    Formula: Recall= TP / TP + FN
    Objective: A high recall indicates that the model is effective at identifying most of the positive instances in the dataset.


